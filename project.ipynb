{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for dataset images and labels\n",
    "data_path = \"path_to_your_dataset\"\n",
    "images_path = os.path.join(data_path, \"images\")\n",
    "labels_path = os.path.join(data_path, \"labels\")\n",
    "\n",
    "# Define thresholds for size categorization (adjust based on dataset analysis)\n",
    "SMALL_THRESHOLD = 10000  # Area below this is considered small\n",
    "LARGE_THRESHOLD = 40000  # Area above this is considered large\n",
    "\n",
    "# Custom dataset class for loading chicken images and annotations\n",
    "class ChickenDataset(Dataset):\n",
    "    def __init__(self, images_path, labels_path, transform=None):\n",
    "        self.images_path = images_path\n",
    "        self.labels_path = labels_path\n",
    "        self.image_files = [f for f in os.listdir(images_path) if f.endswith(\".jpg\")]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.images_path, img_name)\n",
    "        label_path = os.path.join(self.labels_path, img_name.replace(\".jpg\", \".txt\"))\n",
    "\n",
    "        # Load and preprocess image\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        image = cv2.resize(image, (224, 224))  # Resize to match CNN input size\n",
    "\n",
    "        # Load YOLOv8 annotation file\n",
    "        with open(label_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            if not lines:\n",
    "                raise ValueError(f\"No annotations found for {img_name}\")\n",
    "            _, x, y, w, h = map(float, lines[0].strip().split())\n",
    "\n",
    "        # Convert YOLO format (relative coordinates) to absolute pixel values\n",
    "        img_height, img_width, _ = image.shape\n",
    "        w, h = int(w * img_width), int(h * img_height)\n",
    "        bbox_area = w * h  # Compute bounding box area\n",
    "\n",
    "        # Assign size category based on bounding box area\n",
    "        if bbox_area < SMALL_THRESHOLD:\n",
    "            label = 0  # Small\n",
    "        elif bbox_area > LARGE_THRESHOLD:\n",
    "            label = 2  # Large\n",
    "        else:\n",
    "            label = 1  # Medium\n",
    "\n",
    "        # Apply transformations if specified\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Define data augmentation and preprocessing transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip images horizontally\n",
    "    transforms.RandomRotation(15),  # Random rotation to improve model generalization\n",
    "    transforms.ToTensor(),  # Convert to tensor format for PyTorch\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize pixel values\n",
    "])\n",
    "\n",
    "# Load dataset and create a DataLoader for batching\n",
    "dataset = ChickenDataset(images_path, labels_path, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)  # Shuffle for better training\n",
    "\n",
    "# Define CNN model (ResNet18 with modified classification layer)\n",
    "class ChickenSizeClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ChickenSizeClassifier, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)  # Load pre-trained ResNet18\n",
    "        self.model.fc = nn.Linear(512, 3)  # Modify final layer for 3-class classification (Small, Medium, Large)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Function to train the model\n",
    "def train_model():\n",
    "    model = ChickenSizeClassifier()\n",
    "    criterion = nn.CrossEntropyLoss()  # Loss function for classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with learning rate 0.001\n",
    "    model.train()  # Set model to training mode\n",
    "    \n",
    "    for epoch in range(10):  # Train for 10 epochs\n",
    "        running_loss = 0.0\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.float(), labels  # Convert images to float tensors\n",
    "            optimizer.zero_grad()  # Reset gradients before backpropagation\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagate gradients\n",
    "            optimizer.step()  # Update model weights\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Print loss after each epoch to track training progress\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(dataloader)}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), \"chicken_size_model.pth\")  # Save trained model\n",
    "    print(\"Model training complete!\")\n",
    "\n",
    "# Execute training function\n",
    "train_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
